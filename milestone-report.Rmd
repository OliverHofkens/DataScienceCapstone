---
title: "Capstone Milestone Report"
author: "Oliver Hofkens"
date: "10/22/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tm)
library(quanteda)
library(ggplot2)
Sys.setlocale("LC_ALL", "nl_BE.UTF-8")
```

This milestone report will provide some basic data exploration, and outline the 
process of cleaning the data before building a model. Four datasets containing 
blog posts, news articles and tweets were made available: German, English, Finnish,
and Russian. I chose to explore the English dataset.

## Data Exploration

We are presented with 200MB of blog post text, 196MB of news articles, and 159MB
of tweets. Those sizes may not say much, so let's look at some counts:

```{r, cache=TRUE}
# Read the corpus in with tm, then transform it to a Quanteda corpus.
corp <-VCorpus(DirSource('data/raw/final/en_US', encoding="UTF-8"), readerControl = list(reader = readPlain))
corp <- corpus(corp)
nSentences <- nsentence(corp)
nWords <- ntoken(corp)
```

* The blogposts contain a total of `r nSentences[1]` sentences, containing `r nWords[1]` words.  
* The news articles contain `r nSentences[2]` sentences, with `r nWords[2]` words.  
* The tweets contain a total of `r nSentences[3]` sentences, containing `r nWords[3]` words.  

Even though the tweets dataset is the smallest, it contains the largest number of
sentences, with a relatively low amount of words. This can be attributed to the maximum 
of 140 characters in a tweet.  

Let's take a closer look at our data, which words and/or symbols appear the most 
in each of the datasets?

```{r, cache=TRUE}
docFeatureMatrix <- dfm(corp)
textplot_wordcloud(docFeatureMatrix, comparison = TRUE, title.size=1, max.words=200)
```

This wordcloud actually tells us a lot about our data:  

* On Twitter, people like to shout (!), ask questions (?), and have personal conversations (you, I)  
* News articles often quote others (said, quotation marks), mainly important people (president, officials)  
* Blog posts are less predictable, showing more general language features (and, of, to)  
  
Let's have a look at the total word counts accross the datasets:  

```{r}
topFeats <- topfeatures(docFeatureMatrix, n = 50)
df <- data.frame(word = names(topFeats), count = topFeats)
df$word <- as.factor(df$word)
df$word <- reorder(df$word, -df$count)

ggplot(df, aes(x = df$word, y = df$count)) +
    geom_col() +
    theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
    xlab("Word or Symbol") + 
    ylab("Count")
```

This graph actually matches really well to other word counts for the English 
language, such as this graph of Zipf's law, applied to the contents of English Wikipedia:  
![Zipf's law for English Wikipedia pages](http://wugology.com/wp-content/uploads/2014/07/zipfwiki.png)

## Modeling Preparations

The model should be computationally efficient, so the goal is to get a high 
prediction accuracy, without building a complete and correct language model.  
To get to that goal, I intend to do the following preprocessing steps:  

* Lowercase all text. The model will not care about capitalization.
* Remove numbers, or replace them with some sort of number marker. Learning numbers
in sentences is not productive as they will almost always change.
* Remove in-sentence punctuation, such as commas and quotation marks. The model
 does not care about style.
* Replace end-of-sentence punctuation (!.?) with a single 'end-of-string' marker. This 
will allow the model to recognize the ends of sentences, without being too specific.
* Replace swearing, cursing, and other bad words (course requirement).
* Keep only the 10000 most frequent words, and replace the rest with an 'unk' marker.
This allows for a more efficient model, and during prediction we still have a grasp 
of how many words we DON'T know.